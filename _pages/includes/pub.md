
# üìù Selected Publications  

[//]: # (My full paper list is shown at [my Google Scholar]&#40;https://scholar.google.com/citations?user=wIlpfXEAAAAJ&#41;.)

[//]: # (## ü§ñ Computer Agents)


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR'25 Spotlight</div><img src='images/atlas.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[OS-ATLAS: A Foundation Action Model For Generalist GUI Agents](https://arxiv.org/abs/2410.23218) \\
**Zhiyong Wu**, Zhenyu Wu*, Fangzhi Xu*, Yian Wang*, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng Chen, Paul Pu Liang, Yu Qiao.

- Check demos at [**Our Website**](https://osatlas.github.io/) 
- SOTA GUI grounding and action model upon which you can easily build your own agent. [**Code** ![](https://img.shields.io/github/stars/OS-Copilot/OS-Atlas?style=social)](https://github.com/OS-Copilot/OS-Atlas).
- Repost and like us on [Twitter](https://x.com/zywu_hku/status/1853401677367497023)
</div>

</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">LLMAgents@ICLR 2024</div><img src='images/friday.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[OS-Copilot: Towards Generalist Computer Agents with Self-Improvement](https://arxiv.org/pdf/2402.07456.pdf) \\
**Zhiyong Wu**, Chengcheng Han*, Zichen Ding, Zhenmin Weng, Zhoumianze Liu, Shunyu Yao, Tao Yu, Lingpeng Kong.


- Check demos at [**Our Website**](https://os-copilot.github.io/) 
- Build your personal agents at [**Code** ![](https://img.shields.io/github/stars/OS-Copilot/OS-Copilot?style=social)](https://github.com/OS-Copilot/OS-Copilot). 
- Join our [Discord](https://discord.gg/PCeh4XbhjB) to have fun, or follow us on [Twitter](https://twitter.com/oscopilot)
</div>

</div>


[//]: # (<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2021</div><img src='images/fs2.png' alt="sym" width="100%"></div></div>)

[//]: # (<div class='paper-box-text' markdown="1">)

[//]: # ()
[//]: # ([FastSpeech 2: Fast and High-Quality End-to-End Text to Speech]&#40;https://arxiv.org/abs/2006.04558&#41; \\)

[//]: # (**Yi Ren**, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, Tie-Yan Liu)

[//]: # ()
[//]: # ([**Project**]&#40;https://speechresearch.github.io/fastspeech2/&#41; <strong><span class='show_paper_citations' data='4FA6C0AAAAAJ:LkGwnXOMwfcC'></span></strong>)

[//]: # (  - This work is included by many famous speech synthesis open-source projects, such as [PaddlePaddle/Parakeet ![]&#40;https://img.shields.io/github/stars/PaddlePaddle/PaddleSpeech?style=social&#41;]&#40;https://github.com/PaddlePaddle/PaddleSpeech&#41;, [ESPNet ![]&#40;https://img.shields.io/github/stars/espnet/espnet?style=social&#41;]&#40;https://github.com/espnet/espnet&#41; and [fairseq ![]&#40;https://img.shields.io/github/stars/pytorch/fairseq?style=social&#41;]&#40;https://github.com/pytorch/fairseq&#41;.)

[//]: # (</div>)

[//]: # (</div>)

[//]: # ()
[//]: # ()
[//]: # (<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2024</div><img src='images/mega.png' alt="sym" width="100%"></div></div>)

[//]: # (<div class='paper-box-text' markdown="1">)

[//]: # ()
[//]: # ([Mega-TTS 2: Boosting Prompting Mechanisms for Zero-Shot Speech Synthesis]&#40;https://openreview.net/forum?id=mvMI3N4AvD&#41; \\ )

[//]: # (Ziyue Jiang, Jinglin Liu, **Yi Ren**, et al.)

[//]: # ()
[//]: # ([**Project**]&#40;https://boostprompt.github.io/boostprompt/&#41; )

[//]: # (  - This work has been deployed on many TikTok products.)

[//]: # (  - Advandced zero-shot voice cloning model.)

[//]: # (</div>)

[//]: # (</div>)

[//]: # ()
[//]: # ()
[//]: # (<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI 2022</div><img src='images/diffsinger.png' alt="sym" width="100%"></div></div>)

[//]: # (<div class='paper-box-text' markdown="1">)

[//]: # ()
[//]: # ([DiffSinger: Singing Voice Synthesis via Shallow Diffusion Mechanism]&#40;https://arxiv.org/abs/2105.02446&#41; \\)

[//]: # (Jinglin Liu, Chengxi Li, **Yi Ren**, Feiyang Chen, Zhou Zhao)

[//]: # ()
[//]: # (- Many [video demos]&#40;https://www.bilibili.com/video/BV1be411N7JA&#41; created by the [DiffSinger community]&#40;https://github.com/openvpi&#41; are released.)

[//]: # (- DiffSinger was introduced in [a very popular video]&#40;https://www.bilibili.com/video/BV1uM411t7ZJ&#41; &#40;1600k+ views&#41; on Bilibili!)

[//]: # ()
[//]: # (- [**Project**]&#40;https://diffsinger.github.io/&#41; \| [![]&#40;https://img.shields.io/github/stars/NATSpeech/NATSpeech?style=social&label=DiffSpeech Stars&#41;]&#40;https://github.com/NATSpeech/NATSpeech&#41; \| [![]&#40;https://img.shields.io/github/stars/MoonInTheRiver/DiffSinger?style=social&label=DiffSinger Stars&#41;]&#40;https://github.com/MoonInTheRiver/DiffSinger&#41; \| [![Hugging Face]&#40;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-blue?label=Demo&#41;]&#40;https://huggingface.co/spaces/NATSpeech/DiffSpeech&#41;)

[//]: # (</div>)

[//]: # (</div>)

[//]: # ()
[//]: # ()
[//]: # (<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2021</div><img src='images/portaspeech.png' alt="sym" width="100%"></div></div>)

[//]: # (<div class='paper-box-text' markdown="1">)
[//]: # ()
[//]: # ([PortaSpeech: Portable and High-Quality Generative Text-to-Speech]&#40;https://arxiv.org/abs/2109.15166&#41; \\)

[//]: # (**Yi Ren**, Jinglin Liu, Zhou Zhao)
[//]: # ()
[//]: # ([**Project**]&#40;https://portaspeech.github.io/&#41; \| [![]&#40;https://img.shields.io/github/stars/NATSpeech/NATSpeech?style=social&label=Code+Stars&#41;]&#40;https://github.com/NATSpeech/NATSpeech&#41; \| [![Hugging Face]&#40;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-blue?label=Demo&#41;]&#40;https://huggingface.co/spaces/NATSpeech/PortaSpeech&#41;)
[//]: # (</div>)

[//]: # (</div>)

- ``arXiv`` [ScienceBoard: Evaluating Multimodal Autonomous Agents in Realistic Scientific Workflows](https://arxiv.org/abs/2505.19897), Qiushi Sun, Zhoumianze Liu, Chang Ma, Zichen Ding, Fangzhi Xu, Zhangyue Yin, Haiteng Zhao, Zhenyu Wu, Kanzhi Cheng, Zhaoyang Liu, Jianing Wang, Qintong Li, Xiangru Tang, Tianbao Xie, Xiachong Feng, Xiang Li, Ben Kao, Wenhai Wang, Biqing Qi, Lingpeng Kong, **Zhiyong Wu**. [![](https://img.shields.io/github/stars/OS-Copilot/ScienceBoard?style=social&label=Code+Stars)](https://github.com/OS-Copilot/ScienceBoard)
- ``ACL 2025`` [OS-Genesis: Automating GUI Agent Trajectory Construction via Reverse Task Synthesis](https://arxiv.org/pdf/2412.19723), Qiushi Sun, Kanzhi Cheng, Zichen Ding, Chuanyang Jin, Yian Wang, Fangzhi Xu, Zhenyu Wu, Chengyou Jia, Liheng Chen, Zhoumianze Liu, Ben Kao, Guohao Li, Junxian He, Yu Qiao, **Zhiyong Wu**. [![](https://img.shields.io/github/stars/OS-Copilot/OS-Genesis?style=social&label=Code+Stars)](https://github.com/OS-Copilot/OS-Genesis)
- ``ACL 2025`` [Interative Evolution: A Neural-symbolic Self-Training Framework for Large Language Models](https://arxiv.org/abs/2406.11736), Fangzhi Xu, Qiushi Sun, Kanzhi Cheng, Jun Liu, Yu Qiao, **Zhiyong Wu**.[![](https://img.shields.io/github/stars/xufangzhi/ENVISIONS?style=social&label=Code+Stars)](https://github.com/xufangzhi/ENVISIONS)
- ``ACL 2024`` [SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents](https://arxiv.org/abs/2311.09278), Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, **Zhiyong Wu**. [![](https://img.shields.io/github/stars/njucckevin/SeeClick?style=social&label=Code+Stars)](https://github.com/njucckevin/SeeClick)
- ``ACL 2023`` [Self-adaptive In-context Learning](https://arxiv.org/abs/2212.10375), **Zhiyong Wu**, Yaoxiang Wang, Jiacheng Ye, Lingpeng Kong. [![](https://img.shields.io/github/stars/Shark-NLP/OpenICL?style=social&label=Code+Stars)](https://github.com/Shark-NLP/OpenICL)
- `ICLR 2023` [DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models](https://arxiv.org/abs/2210.08933), Shansan Gong, Mukai Li, Jiangtao Feng, **Zhiyong Wu**, Lingpeng Kong [![](https://img.shields.io/github/stars/Shark-NLP/DiffuSeq?style=social&label=Code+Stars)](https://github.com/Shark-NLP/DiffuSeq)
- `ACL 2020`[Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting BERT](https://arxiv.org/abs/2004.14786), **Zhiyong Wu**, Yun Chen, et al. [![](https://img.shields.io/github/stars/LividWo/Perturbed-Masking?style=social&label=Code+Stars)](https://github.com/LividWo/Perturbed-Masking)

[//]: # (- ``ACL 2023 Demo`` [OpenICL: An Open-Source Framework for In-context Learning]&#40;https://arxiv.org/abs/2303.02913&#41;, Zhenyu Wu, YaoXiang Wang, **Zhiyong Wu** et al.  [![]&#40;https://img.shields.io/github/stars/Shark-NLP/OpenICL?style=social&label=Code+Stars&#41;]&#40;https://github.com/Shark-NLP/OpenICL&#41;)
[//]: # (- ``ICML 2023`` [Compositional Exemplars for In-context Learning]&#40;https://arxiv.org/abs/2302.05698&#41;, Jiacheng Ye, **Zhiyong Wu**, Jiangtao Feng, Tao Yu, Lingpeng Kong. )
[//]: # (- `EMNLP 2022` [ZeroGen: Efficient Zero-shot Learning via Dataset Generation]&#40;https://arxiv.org/abs/2202.07922&#41;, Jiacheng Ye*, Jiahui Gao*, Qintong Li, Hang Xu, Jiangtao Feng, **Zhiyong Wu**, Tao Yu and Lingpeng Kong. )
[//]: # (- `ACL 2021` [Good for Misconceived Reasons: An Empirical Revisiting on the Need for Visual Context in Multimodal Machine Translation]&#40;https://arxiv.org/pdf/2105.14462.pdf&#41;, **Zhiyong Wu**, Lingpeng Kong, Wei Bi, Xiang Li, Ben Kao. )

[//]: # (## üëÑ TalkingFace & Avatar)

[//]: # ()
[//]: # ()
[//]: # (## üìö Machine Translation )

[//]: # ()
[//]: # (## üéº Music & Dance Generation )

[//]: # ()
[//]: # (## üßë‚Äçüé® Generative Model)

[//]: # ()
[//]: # (## Others)
